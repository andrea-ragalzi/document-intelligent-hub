# Backend Architecture

The backend is built with FastAPI and provides a robust RAG (Retrieval-Augmented Generation) API for document processing and intelligent Q&A.

## ğŸ“ Directory Structure

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ config.py          # Configuration and settings
â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ chroma_client.py   # ChromaDB vector store client
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ rag_router.py      # API endpoints
â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ rag.py             # Pydantic models for validation
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ rag_service.py     # RAG business logic
â”‚       â””â”€â”€ language_service.py # Language detection/translation
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py            # Pytest fixtures
â”‚   â””â”€â”€ test_rag_endpoints.py  # API endpoint tests
â”œâ”€â”€ chroma_db/                  # ChromaDB persistent storage
â”œâ”€â”€ main.py                     # FastAPI application entry point
â”œâ”€â”€ pyproject.toml              # Python dependencies (Poetry)
â”œâ”€â”€ pytest.ini                  # Pytest configuration
â”œâ”€â”€ Dockerfile                  # Docker container definition
â””â”€â”€ README.md
```

## ğŸ—ï¸ Architecture Layers

### 1. **API Layer** (`routers/`)

**File**: `rag_router.py`

Handles HTTP requests and responses. Defines all API endpoints.

**Endpoints**:
- `GET /` - Health check
- `POST /upload/` - Upload and index PDF documents
- `POST /query/` - Query indexed documents with RAG

**Key Features**:
- FastAPI automatic validation via Pydantic schemas
- CORS middleware for frontend communication
- Error handling with HTTP exceptions
- Multi-part file upload support

### 2. **Service Layer** (`services/`)

Contains business logic separated from API concerns.

#### **RAG Service** (`rag_service.py`)

Core RAG pipeline implementation.

**Key Functions**:
- `index_pdf_document()` - Process and store PDF in vector DB
  - Extract text from PDF
  - Split into chunks with overlap
  - Generate embeddings
  - Store in ChromaDB with metadata
  
- `query_documents()` - Retrieve and generate answers
  - Detect query language
  - Retrieve relevant chunks via semantic search
  - Build context-aware prompt with chat history
  - Generate response using LLM
  - Return answer with sources

**Components Used**:
- **LangChain** - Orchestration framework
- **RecursiveCharacterTextSplitter** - Chunk documents intelligently
- **OpenAIEmbeddings** - Generate vector embeddings
- **ChatOpenAI** - LLM for answer generation

#### **Language Service** (`language_service.py`)

Handles multi-language support.

**Functions**:
- `detect_language()` - Detect language of query
- `translate_to_english()` - Translate non-English queries
- `translate_to_language()` - Translate responses back to original language

### 3. **Data Layer** (`db/`)

#### **ChromaDB Client** (`chroma_client.py`)

Vector database management.

**Features**:
- Persistent storage with disk-based database
- Per-user collections for multi-tenancy
- Metadata filtering for data isolation
- Automatic collection creation

**Key Methods**:
- `get_or_create_collection(user_id)` - Get user-specific collection
- Semantic similarity search
- Document storage with metadata

### 4. **Data Models** (`schemas/`)

**File**: `rag.py`

Pydantic models for request/response validation.

**Models**:

```python
class UploadRequest(BaseModel):
    """File upload request"""
    file: UploadFile
    user_id: str

class UploadResponse(BaseModel):
    """Upload success response"""
    message: str
    status: str
    chunks_indexed: int

class ChatHistoryItem(BaseModel):
    """Single chat history message"""
    type: Literal["user", "assistant"]
    text: str

class QueryRequest(BaseModel):
    """Query request with history"""
    query: str
    user_id: str
    chat_history: List[ChatHistoryItem] = []

class QueryResponse(BaseModel):
    """Query response with sources"""
    answer: str
    source_documents: List[str]
```

### 5. **Configuration** (`core/`)

**File**: `config.py`

Centralized configuration using Pydantic Settings.

**Settings**:
- OpenAI API key
- Model names (embeddings, LLM)
- ChromaDB path
- CORS origins
- Chunk size and overlap

**Example**:
```python
class Settings(BaseSettings):
    openai_api_key: str
    embedding_model: str = "text-embedding-ada-002"
    llm_model: str = "gpt-3.5-turbo"
    chroma_db_path: str = "chroma_db"
    
    class Config:
        env_file = ".env"

settings = Settings()
```

## ğŸ”„ RAG Pipeline Flow

### Document Upload Flow

```
1. User uploads PDF
   â†“
2. FastAPI receives file + user_id
   â†“
3. RAG Service extracts text from PDF
   â†“
4. Text split into chunks (1000 chars, 200 overlap)
   â†“
5. Generate embeddings for each chunk
   â†“
6. Store in ChromaDB with metadata:
   - user_id (for filtering)
   - chunk_id
   - source document name
   â†“
7. Return success response
```

### Query Flow

```
1. User sends query + user_id + chat_history
   â†“
2. Detect query language
   â†“
3. Translate to English if needed
   â†“
4. Retrieve relevant chunks from ChromaDB:
   - Semantic similarity search
   - Filter by user_id (multi-tenant)
   - Top 4 most relevant chunks
   â†“
5. Build context:
   - System prompt
   - Chat history
   - Retrieved chunks
   - Current query
   â†“
6. Send to GPT for answer generation
   â†“
7. Translate response to original language
   â†“
8. Return answer + source chunk IDs
```

## ğŸ” Multi-Tenancy

**Data Isolation Strategy**:

Each user has a separate ChromaDB collection:
- Collection name: `rag_docs_{user_id}`
- No cross-user data leakage
- Metadata filtering ensures user-specific queries

**Security Features**:
- User ID required for all operations
- Collections isolated at DB level
- No shared embeddings between users

## ğŸ§ª Testing

**Framework**: pytest with fixtures

**Test Coverage**:
- âœ… Health check endpoint
- âœ… PDF upload (valid files)
- âœ… Invalid file type rejection
- âœ… Query without chat history
- âœ… Query with chat history
- âœ… Multi-tenant isolation
- âœ… Error handling (missing user_id, invalid data)

**Run Tests**:
```bash
# All tests
pytest

# With coverage
pytest --cov=app --cov-report=html

# Specific test
pytest tests/test_rag_endpoints.py::test_upload_pdf -v
```

## ğŸ“¦ Dependencies

**Core**:
- `fastapi` - Web framework
- `uvicorn` - ASGI server
- `langchain` - LLM orchestration
- `openai` - OpenAI API client
- `chromadb` - Vector database
- `pypdf` - PDF text extraction
- `pydantic` - Data validation

**Dev**:
- `pytest` - Testing framework
- `pytest-cov` - Coverage reporting
- `httpx` - HTTP client for testing

## ğŸš€ Deployment

### Local Development
```bash
cd backend
python -m venv venv
source venv/bin/activate
pip install -e .
uvicorn main:app --reload
```

### Docker
```bash
docker build -t rag-backend .
docker run -p 8000:8000 --env-file .env rag-backend
```

### Production Considerations
- Use Gunicorn with Uvicorn workers
- Set up reverse proxy (Nginx)
- Configure proper CORS origins
- Use environment variables for secrets
- Set up logging and monitoring
- Enable rate limiting
- Use connection pooling for ChromaDB

## ğŸ“Š Performance Optimization

**Current Optimizations**:
- Persistent ChromaDB (disk-based)
- Efficient text chunking with overlap
- Limited context window (top 4 chunks)
- Async FastAPI handlers

**Future Improvements**:
- [ ] Caching layer for frequent queries
- [ ] Batch embedding generation
- [ ] Connection pooling
- [ ] Response streaming (SSE)
- [ ] Query result caching
- [ ] Background job processing for large uploads

## ğŸ” Monitoring & Debugging

**Logging**:
- Console logging enabled
- Log requests/responses in development
- Error tracing with stack traces

**Debug Endpoints**:
- `GET /` - Health check with timestamp
- `GET /docs` - Swagger UI
- `GET /redoc` - ReDoc documentation

**Useful Commands**:
```bash
# Check logs
docker-compose logs -f backend

# Enter container
docker-compose exec backend bash

# Test endpoint
curl -X POST http://localhost:8000/rag/query/ \
  -H "Content-Type: application/json" \
  -d '{"query": "test", "user_id": "user1"}'
```

## ğŸ“š Additional Resources

- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [LangChain Documentation](https://python.langchain.com/)
- [ChromaDB Documentation](https://docs.trychroma.com/)
- [OpenAI API Reference](https://platform.openai.com/docs/)
