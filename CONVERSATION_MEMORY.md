# Conversation Memory System

## Overview

The Document Intelligent Hub implements a **hybrid memory strategy** combining short-term buffer memory with optional long-term summarization. This transforms the RAG system from a "stateless parrot" into a context-aware assistant that maintains conversation coherence across multiple exchanges.

## Architecture

### 1. Short-Term Memory (Buffer) âœ… **IMPLEMENTED**

**Purpose**: Maintain recent conversation context for immediate coherence

**How it works**:
- Frontend captures the last **7 exchanges** (14 messages: 7 user + 7 assistant)
- History is sent with each query to the backend
- Backend injects this history into the LLM prompt **between** system instructions and RAG context

**Flow**:
```
1. User asks question â†’ Frontend loads last 7 exchanges from local state
2. POST /rag/query/ with conversation_history array
3. Backend formats history as:
   User: [question 1]
   Assistant: [answer 1]
   User: [question 2]
   Assistant: [answer 2]
   ...
4. LLM receives: System Prompt + History + RAG Context + New Question
5. Response maintains conversation coherence
```

**Benefits**:
- âœ… Simple implementation
- âœ… Precise context for recent exchanges
- âœ… No additional storage required
- âœ… Works immediately without configuration

**Limitations**:
- Limited to ~14 messages (manageable token cost)
- Older context is lost after 7 exchanges

### 2. Long-Term Memory (Summarization) âœ… **IMPLEMENTED**

**Purpose**: Preserve key facts and context from longer conversations

**How it works**:
- After every **20 messages**, the system triggers automatic summarization
- Backend endpoint `/rag/summarize/` uses LLM to generate a concise summary
- Summary highlights:
  - Key facts the user mentioned about themselves
  - Main topics discussed
  - Important Q&A pairs
  - Ongoing issues or follow-up items
- Summary is stored in Firestore alongside conversation history

**Flow**:
```
1. Conversation reaches 20 messages â†’ Frontend detects threshold
2. POST /rag/summarize/ with full conversation history
3. LLM generates ~200 word summary
4. Summary stored in Firestore: conversations/{id}.summary
5. On conversation load â†’ Summary can be injected into system prompt
```

**Benefits**:
- âœ… Unlimited conversation length support
- âœ… Fixed token cost (summary is always ~200 words)
- âœ… Semantically relevant long-term context
- âœ… Automatic trigger (no user action needed)

**Limitations**:
- Adds ~2-3 seconds of latency during summarization
- Requires LLM call (additional API cost)

## Implementation Details

### Backend Changes

**Files Modified**:

1. **`backend/app/schemas/rag.py`**
   - Added `ConversationMessage` schema (role: "user"|"assistant", content: str)
   - Updated `QueryRequest` to include optional `conversation_history: List[ConversationMessage]`
   - Added `SummarizeRequest` and `SummarizeResponse` schemas

2. **`backend/app/services/rag_service.py`**
   - Modified `answer_query()` to accept `conversation_history` parameter
   - Created `RAG_PROMPT_TEMPLATE_WITH_HISTORY` that injects conversation context
   - Added logic to format history as "User: ... / Assistant: ..." text
   - Implemented `generate_conversation_summary()` method using LLM

3. **`backend/app/routers/rag_router.py`**
   - Updated `/query/` endpoint to pass conversation history to service
   - Added new `/summarize/` endpoint for summary generation

### Frontend Changes

**Files Modified**:

1. **`frontend/hooks/useRAGChat.ts`**
   - Modified query submission to include last 14 messages as `conversation_history`
   - Filters out "thinking" placeholders before sending
   - Maps messages to backend schema format

2. **`frontend/lib/types.ts`**
   - Added optional `summary` field to `SavedConversation` interface
   - Added optional `messageCount` field for tracking summarization triggers

3. **`frontend/hooks/useConversationSummary.ts`** (New)
   - Custom hook for automatic summarization
   - Monitors conversation length and triggers summary generation
   - Debounced to avoid excessive API calls
   - Callback-based for flexible integration

### Prompt Structure

**Without History**:
```
[System Instruction: You are a document compliance assistant...]

Context: [RAG retrieved documents]

Question: [User's current question]
```

**With History** (New):
```
[System Instruction: You are a document compliance assistant...]

Conversation History:
User: What is the capital of France?
Assistant: Paris is the capital of France.
User: What about Italy?

Context: [RAG retrieved documents]

Question: What about Italy?
```

The LLM now understands "What about Italy?" refers to asking for Italy's capital, maintaining conversation flow.

## Configuration

**Tunable Parameters**:

```python
# Backend: rag_service.py
MAX_HISTORY_MESSAGES = 14  # Last 7 exchanges (7 user + 7 assistant)

# Frontend: useConversationSummary.ts
SUMMARIZE_THRESHOLD = 20  # Generate summary every 20 messages
```

**Recommendations**:
- **7 exchanges** is optimal for most cases (balances context vs token cost)
- **20 messages** threshold prevents over-summarization while capturing key points
- Adjust based on your use case:
  - Technical support: Keep 10 exchanges (more detailed troubleshooting)
  - General Q&A: Keep 5 exchanges (sufficient for context)

## Usage

### Automatic (Recommended)

No code changes needed! The system works automatically:
1. User asks questions â†’ Recent history is automatically sent
2. Conversation grows â†’ Summary is automatically generated at 20 messages
3. Summary stored â†’ Available for future retrieval

### Manual Summarization (Optional)

If you want to trigger summarization manually:

```typescript
import { API_BASE_URL } from "@/lib/constants";

const generateSummary = async (history: ChatMessage[]) => {
  const response = await fetch(`${API_BASE_URL}/summarize/`, {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      conversation_history: history.map(msg => ({
        role: msg.type === "user" ? "user" : "assistant",
        content: msg.text,
      })),
    }),
  });
  
  const data = await response.json();
  console.log("Summary:", data.summary);
};
```

## Future Enhancements

### Phase 1: Summary Injection (Pending Implementation)

Currently, summaries are generated and stored but **not yet injected** back into new queries. To complete this:

1. Modify `answer_query()` to accept optional `conversation_summary` parameter
2. Update system prompt to include summary:
   ```
   [System Instruction + User Context Summary]
   
   User Background: [LLM-generated summary]
   
   Conversation History: [Last 7 exchanges]
   ...
   ```

3. Frontend: Load summary from Firestore when resuming conversation

### Phase 2: Semantic Memory Search

Instead of just storing summaries, embed them into ChromaDB:
- Each summary becomes a searchable vector
- On new conversation, retrieve similar past conversations
- Inject relevant historical context: "You previously helped this user with..."

### Phase 3: User Profile Memory

Create persistent user profiles:
- Extract facts across ALL conversations: "User is a DevOps engineer in Italy"
- Store in dedicated Firestore collection: `users/{userId}/profile`
- Always inject profile context for personalized responses

## Testing

### Verify Short-Term Memory

1. Upload a document about Eberron (or any topic)
2. Ask: "What are the districts of Sharn?"
3. Follow up: "Tell me more about the third one"
4. The assistant should understand "third one" refers to the third district mentioned

### Verify Long-Term Memory (Summarization)

1. Have a long conversation (21+ messages)
2. Check browser console for: `ðŸ§  Generating conversation summary`
3. After 2 seconds: `âœ… Summary generated: [text]...`
4. Check Firestore: `conversations/{id}` should have `summary` field

## Performance Impact

**Short-Term Memory**:
- Token increase: ~500-1000 tokens per query (7 exchanges)
- Cost impact: ~$0.001-0.002 per query (GPT-4)
- Latency impact: Negligible (<100ms)

**Long-Term Memory**:
- Summarization: ~2000-3000 tokens processed
- Cost: ~$0.005-0.01 per summary
- Frequency: Every 20 messages (amortized cost is low)
- Latency: 2-3 seconds (async, doesn't block user)

## Conclusion

The hybrid memory system provides the best balance between:
- âœ… **Accuracy**: Recent context is precise and complete
- âœ… **Efficiency**: Token costs are predictable and manageable  
- âœ… **Scalability**: Conversations can be unlimited in length
- âœ… **User Experience**: Natural, coherent multi-turn dialogues

The system is production-ready and requires no additional configuration. Summaries are automatically generated and stored, ready for future injection into the system prompt for even richer context awareness.
